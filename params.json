{
  "name": "Flinking With Word2Vec",
  "tagline": "understanding word2vec and how to implement it on distributed systems",
  "body": "## What2What?\r\n\r\nNatural language still remains a difficult problem for computers to solve. At the very least - even those machines that are quite good with language are still awful conversationalists. In many ways, this makes perfect sense: the formal logic of machines can't quite map onto the ontological fluidity of actual human language. After all, a computer understands by digitizing - fitting the full breadth of variety we encounter in the world within a set small enough to analyze in a reasonable amount of time while meaningful language often defies such boxing. How large is the space that can at once parameterize words like 'love', or 'loyalty'? What does this multidimensional dictionary look like?\r\n\r\nThere is a category of modelling techniques called [word embedding](https://en.wikipedia.org/wiki/Word_embedding) that attempts to answer this question by transforming words into vectors of numbers - embedding linguistic tokens in a continuous space. In essence, an embedding will artificially limit the number of dimensions that can describe a word to some value lower than the entire vocabulary. This 'compression' of the context space can give rise to unexpected clusterings of words that can yield useful insights that express linguistic concepts - such as semantic, syntactic or morphological relationships - with spatial analysis.\r\n\r\n**Word2Vec**, like many other embedding techniques, uses a context sensitive neural network architecture to learn word embeddings that can reveal surprisingly rich relationships - such as analogies like King:Queen::Man:Woman - in ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}
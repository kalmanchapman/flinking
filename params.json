{
  "name": "Flinking With Word2Vec",
  "tagline": "understanding word2vec and how to implement it on distributed systems",
  "body": "# Flinking With Word2Vec\r\n\r\nCheck out my [implementation](https://github.com/kalmanchapman/flink/blob/word2vec/flink-libraries/flink-ml/src/main/scala/org/apache/flink/ml/optimization/ContextEmbedder.scala), [presentation](http://bitly.com/flinkingword2vec) and [explanation](https://kalmanchapman.github.io/flinking) \r\n\r\nWhile a fellow at Insight Data Science, I came across a [feature request](https://issues.apache.org/jira/browse/FLINK-2094) for an implementation of Word2Vec for the [Flink distributed processing platform](http://flink.apache.org/). I hadn't really done much with Flink before, but I had played around with the [Word2Vec implementation from the Spark framework](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.Word2Vec) - and as an intrepid engineer with a background in linguistics and an interest in feature learning technologies, this was a perfect reason to dive deeply into understanding this fascinating learning algorithm in the context of a distributed compute framework.\r\n\r\nRead on for more details on Word2Vec and what it looks like in a distributed environment\r\n\r\n## What2What?\r\n\r\nNatural language remains a difficult problem for computers to solve. At the very least - even those machines that arr good with language are still awful conversationalists. In many ways, this makes perfect sense: the formal logic of machines can't quite map onto the ontological fluidity of actual human language. After all, a computer understands by digitizing - fitting the full breadth of variety we encounter in the world within a set small enough to analyze in a reasonable amount of time, while meaningful language often defies such boxing. How large is the space that can at once parameterize words like 'love', or 'loyalty'? What does this multidimensional dictionary look like?\r\n\r\nThere is a category of modelling techniques called [word embedding](https://en.wikipedia.org/wiki/Word_embedding) that attempts to answer this question by transforming words into vectors of numbers - embedding linguistic tokens in a continuous space. In essence, an embedding will artificially limit the number of dimensions that can describe a word to some value lower than the entire vocabulary. This 'compression' of the context space can give rise to linear interrelationships that can yield useful insights into linguistic concepts that might otherwise be intractable to a machine. Cosine distances become synonyms, displacement vectors become verb conjugations, and some spatial translations can even yield valid analogies!\r\n\r\n**Word2Vec** is an approach to word embedding that has created quite a splash since [it was first introduced in 2013](https://arxiv.org/pdf/1301.3781v3.pdf). The popularity of Word2Vec can probably be attributed to its efficient architecture and implementation - as well as the somewhat *mysterious* features underlying the generated vectors.\r\n\r\nAlthough it is implemented with a back-propagated artificial neural network it can quickly learn over huge datasets due to clever data pre-processing, ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}